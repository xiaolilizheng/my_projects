{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed06e5bf",
   "metadata": {},
   "source": [
    "## Part 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07fef7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/zheng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/zheng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# import gensim\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3a2bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data into dataframe\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield json.loads(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('/Users/zheng/Desktop/dataset/Software.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40a3d517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>03 11, 2014</td>\n",
       "      <td>A240ORQ2LF9LUI</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Michelle W</td>\n",
       "      <td>The materials arrived early and were in excell...</td>\n",
       "      <td>Material Great</td>\n",
       "      <td>1394496000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>02 23, 2014</td>\n",
       "      <td>A1YCCU0YRLS0FE</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Rosalind White Ames</td>\n",
       "      <td>I am really enjoying this book with the worksh...</td>\n",
       "      <td>Health</td>\n",
       "      <td>1393113600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>02 17, 2014</td>\n",
       "      <td>A1BJHRQDYVAY2J</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Allan R. Baker</td>\n",
       "      <td>IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...</td>\n",
       "      <td>ARE YOU KIDING ME?</td>\n",
       "      <td>1392595200</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>02 17, 2014</td>\n",
       "      <td>APRDVZ6QBIQXT</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Lucy</td>\n",
       "      <td>This book was missing pages!!! Important pages...</td>\n",
       "      <td>missing pages!!</td>\n",
       "      <td>1392595200</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>10 14, 2013</td>\n",
       "      <td>A2JZTTBSLS1QXV</td>\n",
       "      <td>0077775473</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Albert V.</td>\n",
       "      <td>I have used LearnSmart and can officially say ...</td>\n",
       "      <td>Best study product out there!</td>\n",
       "      <td>1381708800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime      reviewerID        asin  \\\n",
       "0      4.0      True  03 11, 2014  A240ORQ2LF9LUI  0077613252   \n",
       "1      4.0      True  02 23, 2014  A1YCCU0YRLS0FE  0077613252   \n",
       "2      1.0      True  02 17, 2014  A1BJHRQDYVAY2J  0077613252   \n",
       "3      3.0      True  02 17, 2014   APRDVZ6QBIQXT  0077613252   \n",
       "4      5.0     False  10 14, 2013  A2JZTTBSLS1QXV  0077775473   \n",
       "\n",
       "                        style         reviewerName  \\\n",
       "0  {'Format:': ' Loose Leaf'}           Michelle W   \n",
       "1  {'Format:': ' Loose Leaf'}  Rosalind White Ames   \n",
       "2  {'Format:': ' Loose Leaf'}       Allan R. Baker   \n",
       "3  {'Format:': ' Loose Leaf'}                 Lucy   \n",
       "4                         NaN            Albert V.   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  The materials arrived early and were in excell...   \n",
       "1  I am really enjoying this book with the worksh...   \n",
       "2  IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...   \n",
       "3  This book was missing pages!!! Important pages...   \n",
       "4  I have used LearnSmart and can officially say ...   \n",
       "\n",
       "                         summary  unixReviewTime vote image  \n",
       "0                 Material Great      1394496000  NaN   NaN  \n",
       "1                         Health      1393113600  NaN   NaN  \n",
       "2             ARE YOU KIDING ME?      1392595200    7   NaN  \n",
       "3                missing pages!!      1392595200    3   NaN  \n",
       "4  Best study product out there!      1381708800  NaN   NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31676622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "overall                0\n",
       "verified               0\n",
       "reviewTime             0\n",
       "reviewerID             0\n",
       "asin                   0\n",
       "style             225035\n",
       "reviewerName          24\n",
       "reviewText            66\n",
       "summary               56\n",
       "unixReviewTime         0\n",
       "vote              331583\n",
       "image             457928\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing value\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ed870f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove missing value\n",
    "df.dropna(subset=['reviewText'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa7fbc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4ae595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the first 20000 data as our training data\n",
    "data = df.loc[:19999, 'reviewText'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db15cee5",
   "metadata": {},
   "source": [
    "## Part 2: Tokenizing and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd2ac1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We use 183 stop-words from nltk library.\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# use nltk's English stopwords.\n",
    "stopwords = nltk.corpus.stopwords.words('english') #stopwords.append(\"n't\")\n",
    "stopwords.append(\"'s\")\n",
    "stopwords.append(\"'m\")\n",
    "stopwords.append(\"n\") \n",
    "stopwords.append(\"software\")\n",
    "\n",
    "print (\"We use \" + str(len(stopwords)) + \" stop-words from nltk library.\")\n",
    "print (stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9be42ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# tokenization and stemming\n",
    "def tokenization_and_stemming(text):\n",
    "    tokens = []\n",
    "    # exclude stop words and tokenize the document, generate a list of string \n",
    "    for word in nltk.word_tokenize(text):\n",
    "        if word.lower() not in stopwords:\n",
    "            tokens.append(word.lower())\n",
    "\n",
    "    filtered_tokens = []\n",
    "    \n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if token.isalpha():\n",
    "            filtered_tokens.append(token)\n",
    "            \n",
    "    # stemming\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cd4bf03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['materi',\n",
       " 'arriv',\n",
       " 'earli',\n",
       " 'excel',\n",
       " 'condit',\n",
       " 'howev',\n",
       " 'money',\n",
       " 'spent',\n",
       " 'realli',\n",
       " 'come',\n",
       " 'binder',\n",
       " 'loos',\n",
       " 'leaf']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenization_and_stemming(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af4c39cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The materials arrived early and were in excellent condition.  However for the money spent they really should've come with a binder and not just loose leaf.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b0bab9",
   "metadata": {},
   "source": [
    "## Part 3: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8d973fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afterward', 'alon', 'alreadi', 'alway', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becom', 'besid', 'cri', 'describ', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'otherwis', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, there are 20000 reviews and 823 terms.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# define vectorizer parameters\n",
    "# TfidfVectorizer will help us to create tf-idf matrix\n",
    "# max_df : maximum document frequency for the given word\n",
    "# min_df : minimum document frequency for the given word\n",
    "# max_features: maximum number of words\n",
    "# use_idf: if not true, we only calculate tf\n",
    "# stop_words : built-in stop words\n",
    "# tokenizer: how to tokenize the document\n",
    "# ngram_range: (min_value, max_value), eg. (1, 3) means the result will include 1-gram, 2-gram, 3-gram\n",
    "tfidf_model = TfidfVectorizer(max_df=0.99, max_features=1000,\n",
    "                              min_df=0.01, stop_words='english',\n",
    "                              use_idf=True, tokenizer=tokenization_and_stemming, ngram_range=(1,1))\n",
    "\n",
    "tfidf_matrix = tfidf_model.fit_transform(data) #fit the vectorizer to synopses\n",
    "\n",
    "print (\"In total, there are \" + str(tfidf_matrix.shape[0]) + \\\n",
    "      \" reviews and \" + str(tfidf_matrix.shape[1]) + \" terms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b25cce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the words identified by TF-IDF\n",
    "tf_selected_words = tfidf_model.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90ddd4",
   "metadata": {},
   "source": [
    "## Part 4: K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08772d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 4\n",
    "\n",
    "# number of clusters\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d893545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DataFrame films from all of the input files.\n",
    "product = { 'review': df[:20000].reviewText, 'cluster': clusters}\n",
    "frame = pd.DataFrame(product, columns = ['review', 'cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f015a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The materials arrived early and were in excell...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am really enjoying this book with the worksh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This book was missing pages!!! Important pages...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have used LearnSmart and can officially say ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Strong backgroung, good read, quite up to date...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>If you live on Mars and never heard of the int...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i got this book on amazon and it ended up savi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I was very happy with this purchase because th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Recieved in a timely manner- book in great con...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  cluster\n",
       "0  The materials arrived early and were in excell...        0\n",
       "1  I am really enjoying this book with the worksh...        0\n",
       "2  IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...        0\n",
       "3  This book was missing pages!!! Important pages...        0\n",
       "4  I have used LearnSmart and can officially say ...        0\n",
       "5  Strong backgroung, good read, quite up to date...        0\n",
       "6  If you live on Mars and never heard of the int...        0\n",
       "7  i got this book on amazon and it ended up savi...        0\n",
       "8  I was very happy with this purchase because th...        0\n",
       "9  Recieved in a timely manner- book in great con...        0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0a918d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews included in each cluster:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster\n",
       "0     9445\n",
       "1     5537\n",
       "2     2816\n",
       "3     2202"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Number of reviews included in each cluster:\")\n",
    "frame['cluster'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a4892a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Document clustering result by K-means>\n",
      "Cluster 0 words:use,great,program,work,product,good,learn,easi,offic,like,need,price,love,word,version,\n",
      "Cluster 1 words:instal,product,problem,use,program,work,support,comput,version,norton,tri,year,time,quicken,money,\n",
      "Cluster 2 words:game,play,love,fun,kid,old,great,like,nanci,enjoy,year,realli,son,daughter,time,\n",
      "Cluster 3 words:window,xp,os,instal,work,run,comput,mac,upgrad,use,version,microsoft,program,new,oper,\n"
     ]
    }
   ],
   "source": [
    "print (\"<Document clustering result by K-means>\")\n",
    "\n",
    "#km.cluster_centers_ denotes the importances of each items in centroid.\n",
    "#We need to sort it in decreasing-order and get the top k items.\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "Cluster_keywords_summary = {}\n",
    "for i in range(num_clusters):\n",
    "    print (\"Cluster \" + str(i) + \" words:\", end='')\n",
    "    Cluster_keywords_summary[i] = []\n",
    "    for ind in order_centroids[i, :15]: #replace 15 with n words per cluster\n",
    "        Cluster_keywords_summary[i].append(tf_selected_words[ind])\n",
    "        print (tf_selected_words[ind] + \",\", end='')\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05a609a",
   "metadata": {},
   "source": [
    "## Part 5: Topic Modeling - Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d2ff0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LDA for clustering\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90aa65aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 4)\n",
      "[[0.79726549 0.06784448 0.0667019  0.06818813]\n",
      " [0.08623944 0.08574947 0.08157559 0.7464355 ]\n",
      " [0.08592674 0.76887107 0.07362226 0.07157992]\n",
      " ...\n",
      " [0.05028674 0.05070362 0.84912375 0.04988589]\n",
      " [0.03569159 0.03561892 0.89374432 0.03494517]\n",
      " [0.0625006  0.06363495 0.81292128 0.06094317]]\n"
     ]
    }
   ],
   "source": [
    "# document topic matrix for tfidf_matrix_lda\n",
    "lda_output = lda.fit_transform(tfidf_matrix)\n",
    "print(lda_output.shape)\n",
    "print(lda_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b42cd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 823)\n",
      "[[4.77449625e+00 4.02327889e+01 6.34308017e+00 ... 2.62205389e-01\n",
      "  1.23399218e+02 5.78202220e+00]\n",
      " [4.33072736e+01 7.77283213e+01 2.11740694e+01 ... 1.45897631e+01\n",
      "  1.01041262e+02 1.72995067e+01]\n",
      " [1.33864229e+01 7.27598675e+01 2.79425947e+01 ... 4.09676217e+02\n",
      "  1.33089350e+02 3.25856845e+01]\n",
      " [7.07239518e+00 2.89984850e+01 2.66628529e+01 ... 6.66798349e-01\n",
      "  1.51486783e+02 8.60959494e+00]]\n"
     ]
    }
   ],
   "source": [
    "# topics and words matrix\n",
    "topic_word = lda.components_\n",
    "print(topic_word.shape)\n",
    "print(topic_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b49d2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic0</th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc0</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.75</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc5</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc6</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc7</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc8</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc9</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic0  Topic1  Topic2  Topic3  topic\n",
       "Doc0    0.80    0.07    0.07    0.07      0\n",
       "Doc1    0.09    0.09    0.08    0.75      3\n",
       "Doc2    0.09    0.77    0.07    0.07      1\n",
       "Doc3    0.07    0.79    0.07    0.07      1\n",
       "Doc4    0.67    0.23    0.05    0.05      0\n",
       "Doc5    0.07    0.79    0.07    0.07      1\n",
       "Doc6    0.56    0.08    0.28    0.08      0\n",
       "Doc7    0.80    0.07    0.06    0.07      0\n",
       "Doc8    0.74    0.08    0.09    0.09      0\n",
       "Doc9    0.77    0.08    0.07    0.07      0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column names\n",
    "topic_names = [\"Topic\" + str(i) for i in range(lda.n_components)]\n",
    "\n",
    "# index names\n",
    "doc_names = [\"Doc\" + str(i) for i in range(len(data))]\n",
    "\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topic_names, index=doc_names)\n",
    "\n",
    "# get dominant topic for each document\n",
    "topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['topic'] = topic\n",
    "\n",
    "df_document_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a75c756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic\n",
       "2   7137\n",
       "1   5842\n",
       "3   3639\n",
       "0   3382"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_document_topic['topic'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "657bf582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.77449625e+00 4.02327889e+01 6.34308017e+00 ... 2.62205389e-01\n",
      "  1.23399218e+02 5.78202220e+00]\n",
      " [4.33072736e+01 7.77283213e+01 2.11740694e+01 ... 1.45897631e+01\n",
      "  1.01041262e+02 1.72995067e+01]\n",
      " [1.33864229e+01 7.27598675e+01 2.79425947e+01 ... 4.09676217e+02\n",
      "  1.33089350e+02 3.25856845e+01]\n",
      " [7.07239518e+00 2.89984850e+01 2.66628529e+01 ... 6.66798349e-01\n",
      "  1.51486783e+02 8.60959494e+00]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>activ</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>...</th>\n",
       "      <th>wors</th>\n",
       "      <th>worst</th>\n",
       "      <th>worth</th>\n",
       "      <th>write</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>x</th>\n",
       "      <th>xp</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic0</th>\n",
       "      <td>4.774496</td>\n",
       "      <td>40.232789</td>\n",
       "      <td>6.343080</td>\n",
       "      <td>3.977757</td>\n",
       "      <td>1.304352</td>\n",
       "      <td>3.605419</td>\n",
       "      <td>0.255138</td>\n",
       "      <td>57.254499</td>\n",
       "      <td>27.277314</td>\n",
       "      <td>7.724054</td>\n",
       "      <td>...</td>\n",
       "      <td>1.936370</td>\n",
       "      <td>3.522369</td>\n",
       "      <td>28.840771</td>\n",
       "      <td>25.422972</td>\n",
       "      <td>13.700701</td>\n",
       "      <td>12.746349</td>\n",
       "      <td>0.271749</td>\n",
       "      <td>0.262205</td>\n",
       "      <td>123.399218</td>\n",
       "      <td>5.782022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>43.307274</td>\n",
       "      <td>77.728321</td>\n",
       "      <td>21.174069</td>\n",
       "      <td>13.514197</td>\n",
       "      <td>39.187045</td>\n",
       "      <td>101.151444</td>\n",
       "      <td>66.290121</td>\n",
       "      <td>5.170356</td>\n",
       "      <td>48.584867</td>\n",
       "      <td>47.359077</td>\n",
       "      <td>...</td>\n",
       "      <td>19.529603</td>\n",
       "      <td>22.658433</td>\n",
       "      <td>61.812335</td>\n",
       "      <td>63.515155</td>\n",
       "      <td>26.943935</td>\n",
       "      <td>34.161135</td>\n",
       "      <td>8.623621</td>\n",
       "      <td>14.589763</td>\n",
       "      <td>101.041262</td>\n",
       "      <td>17.299507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>13.386423</td>\n",
       "      <td>72.759868</td>\n",
       "      <td>27.942595</td>\n",
       "      <td>18.259203</td>\n",
       "      <td>72.891816</td>\n",
       "      <td>22.467353</td>\n",
       "      <td>11.593717</td>\n",
       "      <td>58.388292</td>\n",
       "      <td>47.033059</td>\n",
       "      <td>37.984279</td>\n",
       "      <td>...</td>\n",
       "      <td>25.428110</td>\n",
       "      <td>32.206209</td>\n",
       "      <td>62.409665</td>\n",
       "      <td>22.236962</td>\n",
       "      <td>11.729091</td>\n",
       "      <td>34.652372</td>\n",
       "      <td>76.524066</td>\n",
       "      <td>409.676217</td>\n",
       "      <td>133.089350</td>\n",
       "      <td>32.585684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>7.072395</td>\n",
       "      <td>28.998485</td>\n",
       "      <td>26.662853</td>\n",
       "      <td>0.642503</td>\n",
       "      <td>0.716284</td>\n",
       "      <td>0.256264</td>\n",
       "      <td>3.130665</td>\n",
       "      <td>40.051916</td>\n",
       "      <td>32.424947</td>\n",
       "      <td>5.924500</td>\n",
       "      <td>...</td>\n",
       "      <td>1.222534</td>\n",
       "      <td>4.880821</td>\n",
       "      <td>28.546327</td>\n",
       "      <td>8.169370</td>\n",
       "      <td>2.315292</td>\n",
       "      <td>10.699998</td>\n",
       "      <td>0.257165</td>\n",
       "      <td>0.666798</td>\n",
       "      <td>151.486783</td>\n",
       "      <td>8.609595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 823 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             abil        abl    absolut     accept     access     account  \\\n",
       "Topic0   4.774496  40.232789   6.343080   3.977757   1.304352    3.605419   \n",
       "Topic1  43.307274  77.728321  21.174069  13.514197  39.187045  101.151444   \n",
       "Topic2  13.386423  72.759868  27.942595  18.259203  72.891816   22.467353   \n",
       "Topic3   7.072395  28.998485  26.662853   0.642503   0.716284    0.256264   \n",
       "\n",
       "              act      activ     actual         ad  ...       wors      worst  \\\n",
       "Topic0   0.255138  57.254499  27.277314   7.724054  ...   1.936370   3.522369   \n",
       "Topic1  66.290121   5.170356  48.584867  47.359077  ...  19.529603  22.658433   \n",
       "Topic2  11.593717  58.388292  47.033059  37.984279  ...  25.428110  32.206209   \n",
       "Topic3   3.130665  40.051916  32.424947   5.924500  ...   1.222534   4.880821   \n",
       "\n",
       "            worth      write    written      wrong          x          xp  \\\n",
       "Topic0  28.840771  25.422972  13.700701  12.746349   0.271749    0.262205   \n",
       "Topic1  61.812335  63.515155  26.943935  34.161135   8.623621   14.589763   \n",
       "Topic2  62.409665  22.236962  11.729091  34.652372  76.524066  409.676217   \n",
       "Topic3  28.546327   8.169370   2.315292  10.699998   0.257165    0.666798   \n",
       "\n",
       "              year        yes  \n",
       "Topic0  123.399218   5.782022  \n",
       "Topic1  101.041262  17.299507  \n",
       "Topic2  133.089350  32.585684  \n",
       "Topic3  151.486783   8.609595  \n",
       "\n",
       "[4 rows x 823 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# topic word matrix\n",
    "print(lda.components_)\n",
    "# topic-word matrix\n",
    "df_topic_words = pd.DataFrame(lda.components_)\n",
    "\n",
    "# column and index\n",
    "df_topic_words.columns = tfidf_model.get_feature_names_out()\n",
    "df_topic_words.index = topic_names\n",
    "\n",
    "df_topic_words.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
